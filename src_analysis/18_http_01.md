# workflow 源码解析 04 : http 

我们先看最简单的http例子

https://github.com/chanchann/workflow_annotation/blob/main/demos/07_http/http_req.cc

## create_http_task

首先我们创建出http task

```cpp
WFHttpTask *WFTaskFactory::create_http_task(const std::string& url,
											int redirect_max,
											int retry_max,
											http_callback_t callback)
{
	auto *task = new ComplexHttpTask(redirect_max,
									 retry_max,
									 std::move(callback));
	ParsedURI uri;

	URIParser::parse(url, uri);
	task->init(std::move(uri));
	task->set_keep_alive(HTTP_KEEPALIVE_DEFAULT);
	return task;
}
```

## ComplexHttpTask

我们实际上是new了ComplexHttpTask

```cpp
class ComplexHttpTask : public WFComplexClientTask<HttpRequest, HttpResponse>
{
public:
	ComplexHttpTask(int redirect_max,
					int retry_max,
					http_callback_t&& callback):
		WFComplexClientTask(retry_max, std::move(callback)),
		redirect_max_(redirect_max),
		redirect_count_(0)
	{
		...
	}

protected:
	virtual CommMessageOut *message_out();
	virtual CommMessageIn *message_in();
	virtual int keep_alive_timeout();
	virtual bool init_success();
	virtual void init_failed();
	virtual bool finish_once();

protected:
	bool need_redirect(ParsedURI& uri);
	bool redirect_url(HttpResponse *client_resp, ParsedURI& uri);
	void set_empty_request();

private:
	int redirect_max_;  
	int redirect_count_;
};
```

我们在这里添加了http相关的功能

这些重要的功能后面用到再细说

### WFComplexClientTask<HttpRequest, HttpResponse>

而ComplexHttpTask继承自WFComplexClientTask<HttpRequest, HttpResponse>

已经是用了相应的protocol实例化

```cpp

template<class REQ, class RESP, typename CTX = bool>
class WFComplexClientTask : public WFClientTask<REQ, RESP>
{
protected:
	using task_callback_t = std::function<void (WFNetworkTask<REQ, RESP> *)>;

public:
	WFComplexClientTask(int retry_max, task_callback_t&& cb):
		WFClientTask<REQ, RESP>(NULL, WFGlobal::get_scheduler(), std::move(cb))
	{
		type_ = TT_TCP;
		fixed_addr_ = false;
		retry_max_ = retry_max;
		retry_times_ = 0;
		redirect_ = false;
		ns_policy_ = NULL;
		router_task_ = NULL;
	}

protected:
	// new api for children
	virtual bool init_success() { return true; }
	virtual void init_failed() {}
	virtual bool check_request() { return true; }
	virtual WFRouterTask *route();
	virtual bool finish_once() { return true; }

public:
	void init(const ParsedURI& uri);
	void init(ParsedURI&& uri);
	void init(TransportType type,
			  const struct sockaddr *addr,
			  socklen_t addrlen,
			  const std::string& info);

	void set_transport_type(TransportType type);

	TransportType get_transport_type() const { return type_; }

	virtual const ParsedURI *get_current_uri() const { return &uri_; }

	void set_redirect(const ParsedURI& uri);

	void set_redirect(TransportType type, const struct sockaddr *addr,
					  socklen_t addrlen, const std::string& info);

protected:
	void set_info(const std::string& info);
	void set_info(const char *info);

	virtual void dispatch();
	virtual SubTask *done();

	void clear_resp();
	void disable_retry();


	TransportType type_;
	ParsedURI uri_;
	std::string info_;
	bool fixed_addr_;
	bool redirect_;
	CTX ctx_;
	int retry_max_;
	int retry_times_;
	WFNSPolicy *ns_policy_;
	WFRouterTask *router_task_;
	RouteManager::RouteResult route_result_;
	WFNSTracing tracing_;

public:
	CTX *get_mutable_ctx() { return &ctx_; }

private:
	void clear_prev_state();
	void init_with_uri();
	bool set_port();
	void router_callback(WFRouterTask *task);
	void switch_callback(WFTimerTask *task);
};

```

这一层实现了应用层client该有的功能

最为核心的的是SubTask要求用户实现两个接口，dispatch和done, 在WFComplexClientTask中实现。

我们first->dispatch()调用的就是这个地方

```cpp
template<class REQ, class RESP, typename CTX>
void WFComplexClientTask<REQ, RESP, CTX>::dispatch()
{
	switch (this->state)
	{
	case WFT_STATE_UNDEFINED:  
		if (this->check_request())  
		{
			if (this->route_result_.request_object)  
			{
	case WFT_STATE_SUCCESS:
				this->set_request_object(route_result_.request_object);
				this->WFClientTask<REQ, RESP>::dispatch();
				return;
			}
			router_task_ = this->route();
			series_of(this)->push_front(this);
			series_of(this)->push_front(router_task_);
		}

	default:
		break;
	}

	this->subtask_done();
}
```

我们知道了怎么插入dns解析这一部分

但是dns部分我们忽略，等到dns章节解析时候再来详细分析,

这一部分看似简单，其实很复杂

调用图可见此

![pic](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/http01.png?raw=true)

从图中我们可知WFRsolverTask dispatch，创建dns task进行域名解析

而右边部分则是我们http 请求任务了

看名字就可以看出，从CommRequest::dispatch就是发送req

我们gdb跟踪一下这个函数, 第一次bt发现是WFResolverTask, 第二次还是WFResolverTask

todo : 仔细分析为什么是两次

第三次才算到发消息

```
#0  CommRequest::dispatch (this=0x55555586fad0) at /home/ysy/workflow/src/kerne
l/CommRequest.cc:38
#1  0x00005555555d8112 in Communicator::handle_incoming_reply (this=0x555555858
448 <__CommManager::get_instance()::kInstance+8>, res=0x7fffe4000dc0) at /home/
ysy/workflow/src/kernel/Communicator.cc:691
#2  0x00005555555d823d in Communicator::handle_read_result (this=this@entry=0x5
55555858448 <__CommManager::get_instance()::kInstance+8>, res=res@entry=0x7fffe
4000dc0) at /home/ysy/workflow/src/kernel/Communicator.cc:708
#3  0x00005555555d9dfb in Communicator::handler_thread_routine (context=context
@entry=0x555555858448 <__CommManager::get_instance()::kInstance+8>) at /home/ys
y/workflow/src/kernel/Communicator.cc:1093
#4  0x00005555555f0cdd in __thrdpool_routine (arg=0x55555586c380) at /home/ysy/
workflow/src/kernel/thrdpool.c:72
#5  0x00007ffff7bbb6db in start_thread (arg=0x7ffff4523700) at pthread_create.c
:463
#6  0x00007ffff6beb71f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S
:95
```

我们这里自己思考下，如果我们要发get请求，怎么做？

我们肯定要先connect，然后把发送请求，因为是异步，我们得把这个接收事件给epoll监听上

所以我们先从CommRequest::dispatch()看起

## CommRequest::dispatch 组成

我们dns解析完后，

```cpp
this->WFClientTask<REQ, RESP>::dispatch();
```

走到此处，实际上调用

```cpp
void CommRequest::dispatch()
{
	// 发送请求
	this->scheduler->request(this, this->object, this->wait_timeout,
								 &this->target);
	...
}

```

首先这里关注两点

1. 首先CommRequest 继承自SubTask和CommSession

说明CommRequest是一个SubTask，又满足CommSession的特性

CommSession是一次req->resp的交互，主要要实现message_in(), message_out()等几个虚函数，让核心知道怎么收发消息

```cpp
class CommRequest : public SubTask, public CommSession
```

2. 这里的scheduler是CommScheduler

看名字像是调度器

## 这个CommRequest，scheduler 是什么时候构造出来的呢？

```cpp
WFComplexClientTask(int retry_max, task_callback_t&& cb):
	WFClientTask<REQ, RESP>(NULL, WFGlobal::get_scheduler(), std::move(cb))
{
	type_ = TT_TCP;
	fixed_addr_ = false;
	retry_max_ = retry_max;
	retry_times_ = 0;
	redirect_ = false;
	ns_policy_ = NULL;
	router_task_ = NULL;
}
```

![networktask](./pics/networktask.png)

```cpp
WFGlobal::get_scheduler() 中创建
```

我们的单例是__CommManager，他是其中的一个组合模式的成员，生命周期相同

```cpp
CommScheduler *WFGlobal::get_scheduler()
{
	return __CommManager::get_instance()->get_scheduler();
}
```

```cpp
class __CommManager
{
public:
	CommScheduler *get_scheduler() { return &scheduler_; }
	...
private:
	__CommManager():
		io_server_(NULL),
		io_flag_(false),
		dns_manager_(NULL),
		dns_flag_(false)
	{
		const auto *settings = __WFGlobal::get_instance()->get_global_settings();
		if (scheduler_.init(settings->poller_threads,
							settings->handler_threads) < 0)
			abort();

		signal(SIGPIPE, SIG_IGN);
	}
private:
	CommScheduler scheduler_;
	...
};
```

所以在生成__CommManager单例的时候，构造了CommScheduler并初始化

之前章节我们就已经跟到这里过，__CommManager单例第一次实例化的时候，CommScheduler init，然后Communicator init, 产生poller线程和线程池，并启动了poller线程。

## CommRequest::dispatch 函数中需要的参数(CommSchedObject，CommTarget)

回到此处

```cpp
void CommRequest::dispatch()
{
	// 发送请求
	this->scheduler->request(this, this->object, this->wait_timeout,
								 &this->target);
	...
}

```

这里就做一件事scheduler->request，其中需要CommSchedObject，CommTarget

![CommSchedObject](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/CommSchedObject.png?raw=true)

从WFNetworkTask中可以看出，CommSchedObject是传进来的，最开始在构造的时候，传入的是NULL

```cpp
WFComplexClientTask(int retry_max, task_callback_t&& cb):
	WFClientTask<REQ, RESP>(NULL, WFGlobal::get_scheduler(), std::move(cb))
{
	...
}
```

而什么时候才初始化呢? 在WFComplexClientTask的dispatch中set_request_object

```cpp
template<class REQ, class RESP, typename CTX>
void WFComplexClientTask<REQ, RESP, CTX>::dispatch()
{
	switch (this->state)
	{
	case WFT_STATE_UNDEFINED:
		if (this->check_request())
		{
			if (this->route_result_.request_object)
			{
	case WFT_STATE_SUCCESS:
				this->set_request_object(route_result_.request_object);
				this->WFClientTask<REQ, RESP>::dispatch();
				return;
			}
	...
}
```

这里如何产生的route_result_.request_object，是通过dns去做的，这里先略过，在dns那一节详细阐述。

可以看出我们给个url，dns解析出来之后，我们有了request的目标了

```cpp
class CommSchedObject
{
private:
	virtual CommTarget *acquire(int wait_timeout) = 0;

protected:
	size_t max_load;
	size_t cur_load;
	...
};
```

从这个类成员可以看出，大概是为了负载均衡而设计的。

## CommTarget

而CommTarget是通讯目标，基本上就是ip+port, 还有两个超时参数。连接池什么的都在target里。

CommTarget是在`scheduler->request()`中生成的

```cpp
int request(CommSession *session, CommSchedObject *object,
				int wait_timeout, CommTarget **target)

```

这里参数是CommTarget **target，是一个传出参数, 是从里面`object->acquire(wait_timeout);` 获取出来的

## 仔细解析scheduler->request

```cpp
int request(CommSession *session, CommSchedObject *object,
			int wait_timeout, CommTarget **target)
{

	...
	*target = object->acquire(wait_timeout);
	...
	this->comm.request(session, *target);

}
```

就做两件事，一件事获取通信target，一件是调用request去发request请求

获取通信target 细节可以看CommSchedObject那一节。

我们直接看看发送请求的部分

```cpp
int Communicator::request(CommSession *session, CommTarget *target)
{
	...
	ret = this->request_idle_conn(session, target);
	while (ret < 0)
	{
		entry = this->launch_conn(session, target);
		if (entry)
		{
			session->conn = entry->conn;
			session->seq = entry->seq++;
			data.operation = PD_OP_CONNECT;
			data.fd = entry->sockfd;
			data.ssl = NULL;
			data.context = entry;
			timeout = session->connect_timeout();
			if (mpoller_add(&data, timeout, this->mpoller) >= 0)
				break;

			this->release_conn(entry);
		}
	}

}
```

### request_idle_conn

我们首先复用连接发送

每一个连接的结构是

```cpp

struct CommConnEntry
{
	struct list_head list;
	CommConnection *conn;
	long long seq;
	int sockfd;
#define CONN_STATE_CONNECTING	0
#define CONN_STATE_CONNECTED	1
#define CONN_STATE_RECEIVING	2
#define CONN_STATE_SUCCESS		3
#define CONN_STATE_IDLE			4
#define CONN_STATE_KEEPALIVE	5
#define CONN_STATE_CLOSING		6
#define CONN_STATE_ERROR		7
	int state;
	int error;
	int ref;
	struct iovec *write_iov;
	SSL *ssl;
	CommSession *session;
	CommTarget *target;
	CommService *service;
	mpoller_t *mpoller;
	/* Connection entry's mutex is for client session only. */
	pthread_mutex_t mutex;
};
```

```cpp
int Communicator::request_idle_conn(CommSession *session, CommTarget *target)
{
	struct CommConnEntry *entry;

	pthread_mutex_lock(&target->mutex);

	entry = this->get_idle_conn(target);
	pthread_mutex_unlock(&target->mutex);

	pthread_mutex_lock(&entry->mutex);

	entry->session = session;
	session->conn = entry->conn;
	session->seq = entry->seq++;
	session->out = session->message_out(); // 这里是 CommMessageOut *ComplexHttpTask::message_out()
	this->send_message(entry);

	pthread_mutex_unlock(&entry->mutex);
}
```

这里就是先找个可复用连接

```cpp
struct CommConnEntry *Communicator::get_idle_conn(CommTarget *target)
{
	struct CommConnEntry *entry;
	list_for_each(pos, &target->idle_list)
	{
		entry = list_entry(pos, struct CommConnEntry, list);
		if (mpoller_set_timeout(entry->sockfd, -1, this->mpoller) >= 0)
		{
			list_del(pos);
			return entry;
		}
	}
}
```

然后调用ComplexHttpTask::message_out(), 用于拼凑req请求，自动添加一些字段。

message_out获得的是往连接上要发的数据。

此处我们在 : https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/04_http_improve.md

已经详细分析过

### send_message

```cpp
int Communicator::send_message(struct CommConnEntry *entry)
{
	struct iovec vectors[ENCODE_IOV_MAX];
	struct iovec *end;
	int cnt;

	cnt = entry->session->out->encode(vectors, ENCODE_IOV_MAX);
	...
	end = vectors + cnt;
	if (!entry->ssl)
	{
		cnt = this->send_message_sync(vectors, cnt, entry);
		if (cnt <= 0)
			return cnt;
	}

	return this->send_message_async(end - cnt, cnt, entry);
}
```

注意，这里的`entry->session->out->encode(vectors, ENCODE_IOV_MAX);`中

协议，需要提供协议的序列化和反序列化方法encode

encode函数在消息被发送之前调用，每条消息只调用一次。

encode函数里，用户需要将消息序列化到一个vector数组，数组元素个数不超过max。目前max的值为8192。

结构体struct iovec定义在请参考系统调用readv和writev。

encode函数正确情况下的返回值在0到max之间，表示消息使用了多少个vector。

encode返回-1表示错误。返回-1时，需要置errno。如果返回值>max，将得到一个EOVERFLOW错误。错误都在callback里得到。

为了性能考虑vector里的iov_base指针指向的内容不会被复制。所以一般指向消息类的成员。

![message_out](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/message_out01.png?raw=true)

这里的encode是HttpMessage的实现。此处先从略，等到HTTP协议解析部分再详解，就是先把消息序列化。

## send_message_sync

那么我们开始发送消息

```cpp
int Communicator::send_message_sync(struct iovec vectors[], int cnt,
									struct CommConnEntry *entry)
{
	CommSession *session = entry->session;

	while (cnt > 0)
	{
		n = writev(entry->sockfd, vectors, cnt <= IOV_MAX ? cnt : IOV_MAX);
		if (n < 0)
			return errno == EAGAIN ? cnt : -1;

		for (i = 0; i < cnt; i++)
		{
			if ((size_t)n >= vectors[i].iov_len)
				n -= vectors[i].iov_len;
			else
			{
				vectors[i].iov_base = (char *)vectors[i].iov_base + n;
				vectors[i].iov_len -= n;
				break;
			}
		}

		vectors += i;
		cnt -= i;
	}

	service = entry->service;
	if (service)
	{
		__sync_add_and_fetch(&entry->ref, 1);
		timeout = session->keep_alive_timeout();
		switch (timeout)
		{
		default:
			mpoller_set_timeout(entry->sockfd, timeout, this->mpoller);
			pthread_mutex_lock(&service->mutex);
			if (service->listen_fd >= 0)
			{
				entry->state = CONN_STATE_KEEPALIVE;
				list_add_tail(&entry->list, &service->alive_list);
				entry = NULL;
			}

			pthread_mutex_unlock(&service->mutex);
			if (entry)
			{
		case 0:
				mpoller_del(entry->sockfd, this->mpoller);
				entry->state = CONN_STATE_CLOSING;
			}
		}
	}
	else
	{
		if (entry->state == CONN_STATE_IDLE)
		{
			timeout = session->first_timeout();
			if (timeout == 0)
				timeout = Communicator::first_timeout_recv(session);
			else
			{
				session->timeout = -1;
				session->begin_time.tv_nsec = -1;
			}

			mpoller_set_timeout(entry->sockfd, timeout, this->mpoller);
		}

		entry->state = CONN_STATE_RECEIVING;
	}

	return 0;
}
```

## 未完待续

如果没有可以复用的连接，我们再去启动，那这个任务添加到poller上去异步connect。

然后我们poller检测出这个事件后

```cpp
__poller_thread_routines 中调用 __poller_handle_connect(node, poller);


static void __poller_handle_connect(struct __poller_node *node,
									poller_t *poller)
{
	socklen_t len = sizeof (int);
	int error;

	if (getsockopt(node->data.fd, SOL_SOCKET, SO_ERROR, &error, &len) < 0)
		error = errno;

	if (__poller_remove_node(node, poller))
		return;
	...

	poller->cb((struct poller_result *)node, poller->ctx);
}
```





