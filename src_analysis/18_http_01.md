#! https://zhuanlan.zhihu.com/p/417422301
# workflow 源码解析 10 : http 01

项目源码 : https://github.com/sogou/workflow

更加详细的源码注释可看 : https://github.com/chanchann/workflow_annotation

我们先看最简单的http例子

https://github.com/chanchann/workflow_annotation/blob/main/demos/07_http/http_req.cc

## create_http_task

首先我们创建出http task

```cpp
WFHttpTask *WFTaskFactory::create_http_task(const std::string& url,
											int redirect_max,
											int retry_max,
											http_callback_t callback)
{
	auto *task = new ComplexHttpTask(redirect_max,
									 retry_max,
									 std::move(callback));
	ParsedURI uri;

	URIParser::parse(url, uri);
	task->init(std::move(uri));
	task->set_keep_alive(HTTP_KEEPALIVE_DEFAULT);
	return task;
}
```

## ComplexHttpTask

我们实际上是new了ComplexHttpTask

```cpp
// 这层加入了相关协议和方法
class ComplexHttpTask : public WFComplexClientTask<HttpRequest, HttpResponse>
{
public:
	ComplexHttpTask(int redirect_max,
					int retry_max,
					http_callback_t&& callback):
		WFComplexClientTask(retry_max, std::move(callback)),
		redirect_max_(redirect_max),
		redirect_count_(0)
	{
		...
	}

protected:
	virtual CommMessageOut *message_out();
	virtual CommMessageIn *message_in();
	virtual int keep_alive_timeout();
	virtual bool init_success();
	virtual void init_failed();
	virtual bool finish_once();

protected:
	bool need_redirect(ParsedURI& uri);
	bool redirect_url(HttpResponse *client_resp, ParsedURI& uri);
	void set_empty_request();

private:
	int redirect_max_;  
	int redirect_count_;
};
```

我们在这里添加了http相关的功能

这些重要的功能后面用到再细说

### WFComplexClientTask<HttpRequest, HttpResponse>

而ComplexHttpTask继承自WFComplexClientTask<HttpRequest, HttpResponse>

已经是用了相应的protocol特化

```cpp
template<class REQ, class RESP, typename CTX = bool>
class WFComplexClientTask : public WFClientTask<REQ, RESP>
{
protected:
	using task_callback_t = std::function<void (WFNetworkTask<REQ, RESP> *)>;

public:
	WFComplexClientTask(int retry_max, task_callback_t&& cb):
		WFClientTask<REQ, RESP>(NULL, WFGlobal::get_scheduler(), std::move(cb))
	{
		type_ = TT_TCP;
		fixed_addr_ = false;
		retry_max_ = retry_max;
		retry_times_ = 0;
		redirect_ = false;
		ns_policy_ = NULL;
		router_task_ = NULL;
	}

protected:
	// new api for children
	virtual bool init_success() { return true; }
	virtual void init_failed() {}
	virtual bool check_request() { return true; }
	virtual WFRouterTask *route();
	virtual bool finish_once() { return true; }

public:
	void init(const ParsedURI& uri);
	void init(ParsedURI&& uri);
	void init(TransportType type,
			  const struct sockaddr *addr,
			  socklen_t addrlen,
			  const std::string& info);

	void set_transport_type(TransportType type);

	TransportType get_transport_type() const { return type_; }

	virtual const ParsedURI *get_current_uri() const { return &uri_; }

	void set_redirect(const ParsedURI& uri);

	void set_redirect(TransportType type, const struct sockaddr *addr,
					  socklen_t addrlen, const std::string& info);

protected:
	void set_info(const std::string& info);
	void set_info(const char *info);

	virtual void dispatch();
	virtual SubTask *done();

	void clear_resp();
	void disable_retry();


	TransportType type_;
	ParsedURI uri_;
	std::string info_;
	bool fixed_addr_;
	bool redirect_;
	CTX ctx_;
	int retry_max_;
	int retry_times_;
	WFNSPolicy *ns_policy_;
	WFRouterTask *router_task_;
	RouteManager::RouteResult route_result_;
	WFNSTracing tracing_;

public:
	CTX *get_mutable_ctx() { return &ctx_; }

private:
	void clear_prev_state();
	void init_with_uri();
	bool set_port();
	void router_callback(WFRouterTask *task);
	void switch_callback(WFTimerTask *task);
};

```

这一层实现了应用层client该有的功能

最为核心的的是SubTask要求用户实现两个接口，dispatch和done, 在WFComplexClientTask中实现。

我们first->dispatch()调用的就是这个地方

```cpp
template<class REQ, class RESP, typename CTX>
void WFComplexClientTask<REQ, RESP, CTX>::dispatch()
{
	switch (this->state)
	{
	case WFT_STATE_UNDEFINED:  
		if (this->check_request())  
		{
			if (this->route_result_.request_object)  
			{
	case WFT_STATE_SUCCESS:
				this->set_request_object(route_result_.request_object);
				this->WFClientTask<REQ, RESP>::dispatch();
				return;
			}
			router_task_ = this->route();
			series_of(this)->push_front(this);
			series_of(this)->push_front(router_task_);
		}

	default:
		break;
	}

	this->subtask_done();
}
```

我们知道了怎么插入dns解析这一部分

但是dns部分我们忽略，等到dns章节解析时候再来详细分析,

这一部分看似简单，其实很复杂

调用图可见此

![Image](https://pic4.zhimg.com/80/v2-0da2d0aff043afea3f7a807daf271957.png)
<!-- ![pic](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/http01.png?raw=true) -->

从图中我们可知WFRsolverTask dispatch，创建dns task进行域名解析

而右边部分则是我们http 请求任务了

看名字就可以看出，从CommRequest::dispatch就是发送req

我们这里自己思考下，如果我们要发get请求，怎么做？

我们肯定要先connect，然后把发送请求，因为是异步，我们得把这个接收事件给epoll监听上

所以我们先从CommRequest::dispatch()看起

## CommRequest::dispatch 组成

我们dns解析完后，

```cpp
this->WFClientTask<REQ, RESP>::dispatch();
```

走到此处，实际上调用的是`WFClientTask<REQ, RESP>`爷爷辈的`dispatch`

```cpp
void CommRequest::dispatch()
{
	// 发送请求
	this->scheduler->request(this, this->object, this->wait_timeout,
								 &this->target);
	...
}

```

首先这里关注两点

1. 首先CommRequest 继承自SubTask和CommSession

说明CommRequest是一个SubTask，又满足CommSession的特性

CommSession是一次req->resp的交互，主要要实现message_in(), message_out()等几个虚函数，让核心知道怎么收发消息

```cpp
class CommRequest : public SubTask, public CommSession
```

2. 这里的scheduler是CommScheduler

看名字像是调度器

## 这个CommRequest，scheduler 是什么时候构造出来的呢？

```cpp
WFComplexClientTask(int retry_max, task_callback_t&& cb):
	WFClientTask<REQ, RESP>(NULL, WFGlobal::get_scheduler(), std::move(cb))
{
	type_ = TT_TCP;
	fixed_addr_ = false;
	retry_max_ = retry_max;
	retry_times_ = 0;
	redirect_ = false;
	ns_policy_ = NULL;
	router_task_ = NULL;
}
```

![Image](https://pic4.zhimg.com/80/v2-f080e8f30043988fd8b6750fb882f6e2.png)

<!-- ![networktask](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/networktask.png?raw=true) -->

```cpp
WFGlobal::get_scheduler() 中创建
```

我们的单例是__CommManager，他是其中的一个组合模式的成员，生命周期相同

```cpp
CommScheduler *WFGlobal::get_scheduler()
{
	return __CommManager::get_instance()->get_scheduler();
}
```

```cpp
class __CommManager
{
public:
	CommScheduler *get_scheduler() { return &scheduler_; }
	...
private:
	__CommManager():
		io_server_(NULL),
		io_flag_(false),
		dns_manager_(NULL),
		dns_flag_(false)
	{
		const auto *settings = __WFGlobal::get_instance()->get_global_settings();
		if (scheduler_.init(settings->poller_threads,
							settings->handler_threads) < 0)
			abort();

		signal(SIGPIPE, SIG_IGN);
	}
private:
	CommScheduler scheduler_;
	...
};
```

所以在生成__CommManager单例的时候，构造了CommScheduler并初始化

之前章节我们就已经跟到这里过，__CommManager单例第一次实例化的时候，CommScheduler init，然后Communicator init, 产生poller线程和线程池，并启动了poller线程。

## CommRequest::dispatch 函数中需要的参数(CommSchedObject，CommTarget)

回到此处

```cpp
void CommRequest::dispatch()
{
	// 发送请求
	this->scheduler->request(this, this->object, this->wait_timeout,
								 &this->target);
	...
}

```

这里就做一件事scheduler->request，其中需要CommSchedObject，CommTarget

![CommSchedObject](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/CommSchedObject.png?raw=true)

从WFNetworkTask中可以看出，CommSchedObject是传进来的，最开始在构造的时候，传入的是NULL

```cpp
WFComplexClientTask(int retry_max, task_callback_t&& cb):
	WFClientTask<REQ, RESP>(NULL, WFGlobal::get_scheduler(), std::move(cb))
{
	...
}
```

而什么时候才初始化呢? 在WFComplexClientTask的dispatch中set_request_object

```cpp
template<class REQ, class RESP, typename CTX>
void WFComplexClientTask<REQ, RESP, CTX>::dispatch()
{
	switch (this->state)
	{
	case WFT_STATE_UNDEFINED:
		if (this->check_request())
		{
			if (this->route_result_.request_object)
			{
	case WFT_STATE_SUCCESS:
				this->set_request_object(route_result_.request_object);
				this->WFClientTask<REQ, RESP>::dispatch();
				return;
			}
	...
}
```

这里如何产生的route_result_.request_object，是通过dns去做的，这里先略过，在dns那一节详细阐述。

可以看出我们给个url，dns解析出来之后，我们有了request的目标了

```cpp
class CommSchedObject
{
private:
	virtual CommTarget *acquire(int wait_timeout) = 0;

protected:
	size_t max_load;
	size_t cur_load;
	...
};
```

从这个类成员可以看出，大概是为了负载均衡而设计的。

我们得到的可能是`CommSchedGroup`, 所以需要一定策略选择一个通信的target。如果是`CommSchedTarget`, 则是`return this`, 这个细节在`CommSched` 专题讲解

## CommTarget

而CommTarget是通讯目标，基本上就是ip+port, 还有两个超时参数。连接池什么的都在target里。

CommTarget是在`scheduler->request()`中生成的

```cpp
int request(CommSession *session, CommSchedObject *object,
				int wait_timeout, CommTarget **target)

```

这里参数是CommTarget **target，是一个传出参数, 是从里面`object->acquire(wait_timeout);` 获取出来的

## 仔细解析scheduler->request

```cpp
int request(CommSession *session, CommSchedObject *object,
			int wait_timeout, CommTarget **target)
{

	...
	*target = object->acquire(wait_timeout);
	...
	this->comm.request(session, *target);

}
```

就做两件事，一件事获取通信target，一件是调用request去发request请求

获取通信target 细节可以看CommSchedObject那一节。

我们直接看看发送请求的部分

```cpp
int Communicator::request(CommSession *session, CommTarget *target)
{
	...
	ret = this->request_idle_conn(session, target);
	while (ret < 0)
	{
		entry = this->launch_conn(session, target);
		if (entry)
		{
			session->conn = entry->conn;
			session->seq = entry->seq++;
			data.operation = PD_OP_CONNECT;
			data.fd = entry->sockfd;
			data.ssl = NULL;
			data.context = entry;
			timeout = session->connect_timeout();
			if (mpoller_add(&data, timeout, this->mpoller) >= 0)
				break;

			this->release_conn(entry);
		}
	}

}
```

### request_idle_conn

我们首先复用连接发送

每一个连接的结构是

```cpp

struct CommConnEntry
{
	struct list_head list;
	CommConnection *conn;
	long long seq;
	int sockfd;
#define CONN_STATE_CONNECTING	0
#define CONN_STATE_CONNECTED	1
#define CONN_STATE_RECEIVING	2
#define CONN_STATE_SUCCESS		3
#define CONN_STATE_IDLE			4
#define CONN_STATE_KEEPALIVE	5
#define CONN_STATE_CLOSING		6
#define CONN_STATE_ERROR		7
	int state;
	int error;
	int ref;
	struct iovec *write_iov;
	SSL *ssl;
	CommSession *session;
	CommTarget *target;
	CommService *service;
	mpoller_t *mpoller;
	/* Connection entry's mutex is for client session only. */
	pthread_mutex_t mutex;
};
```

```cpp
int Communicator::request_idle_conn(CommSession *session, CommTarget *target)
{
	struct CommConnEntry *entry;

	pthread_mutex_lock(&target->mutex);

	entry = this->get_idle_conn(target);
	pthread_mutex_unlock(&target->mutex);

	pthread_mutex_lock(&entry->mutex);

	entry->session = session;
	session->conn = entry->conn;
	session->seq = entry->seq++;
	session->out = session->message_out(); // 这里是 CommMessageOut *ComplexHttpTask::message_out()
	this->send_message(entry);

	pthread_mutex_unlock(&entry->mutex);
}
```

这里就是先找个可复用连接

```cpp
struct CommConnEntry *Communicator::get_idle_conn(CommTarget *target)
{
	struct CommConnEntry *entry;
	list_for_each(pos, &target->idle_list)
	{
		entry = list_entry(pos, struct CommConnEntry, list);
		if (mpoller_set_timeout(entry->sockfd, -1, this->mpoller) >= 0)
		{
			list_del(pos);
			return entry;
		}
	}
}
```

然后调用ComplexHttpTask::message_out(), 用于拼凑req请求，自动添加一些字段。

message_out获得的是往连接上要发的数据。

此处我们在 : https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/04_http_improve.md

已经详细分析过

### send_message

```cpp
int Communicator::send_message(struct CommConnEntry *entry)
{
	struct iovec vectors[ENCODE_IOV_MAX];
	struct iovec *end;
	int cnt;

	cnt = entry->session->out->encode(vectors, ENCODE_IOV_MAX);
	...
	end = vectors + cnt;
	if (!entry->ssl)
	{
		cnt = this->send_message_sync(vectors, cnt, entry);
		if (cnt <= 0)
			return cnt;
	}

	return this->send_message_async(end - cnt, cnt, entry);
}
```

注意，这里的`entry->session->out->encode(vectors, ENCODE_IOV_MAX);`中

协议，需要提供协议的序列化和反序列化方法encode

encode函数在消息被发送之前调用，每条消息只调用一次。

encode函数里，用户需要将消息序列化到一个vector数组，数组元素个数不超过max。目前max的值为8192。

结构体struct iovec定义在请参考系统调用readv和writev。

encode函数正确情况下的返回值在0到max之间，表示消息使用了多少个vector。

encode返回-1表示错误。返回-1时，需要置errno。如果返回值>max，将得到一个EOVERFLOW错误。错误都在callback里得到。

为了性能考虑vector里的iov_base指针指向的内容不会被复制。所以一般指向消息类的成员。

![message_out](https://github.com/chanchann/workflow_annotation/blob/main/src_analysis/pics/message_out01.png?raw=true)

这里的encode是HttpMessage的实现。此处先从略，等到HTTP协议解析部分再详解，就是先把消息序列化。

## send_message_sync

那么我们开始发送消息, 如果数据量很小，直接同步方式发送就可

这里分为几部分

- 第一部分 :

就是writev写到`struct iovec vectors[]`中

```cpp
CommSession *session = entry->session;
CommService *service;
int timeout;
ssize_t n;
int i;

while (cnt > 0)
{
	// On success, readv() and preadv() return the number of bytes read; 
	// writev() and pwritev() return the number of bytes written. 
	// On error, -1 is returned, and errno is set appropriately.
	n = writev(entry->sockfd, vectors, cnt <= IOV_MAX ? cnt : IOV_MAX);
	if (n < 0)
		return errno == EAGAIN ? cnt : -1;

	for (i = 0; i < cnt; i++)
	{
		if ((size_t)n >= vectors[i].iov_len)
			n -= vectors[i].iov_len;
		else
		{
			vectors[i].iov_base = (char *)vectors[i].iov_base + n;
			vectors[i].iov_len -= n;
			break;
		}
	}

	vectors += i;
	cnt -= i;
}
```

- 第二部分

如果entry中有`CommService`, 

主要操作就是

1. entry->ref++

2. mpoller_set_timeout

3. service->listen_fd > 0

如果是服务端的话，加入service的alive_list

list_add_tail(&entry->list, &service->alive_list);

alive_list是service上的成员，保存该serivce上所有keep alive的连接。这个list唯一的作用是drain，就是当连接数达到上限时用于关掉比较久没有使用的连接，以及程序退出的时候关闭所有keep alive连接。

如果是客户端的话，那么就mpoller_del，直接发送就可(todo)

```cpp
service = entry->service;
if (service)
{
	__sync_add_and_fetch(&entry->ref, 1);
	timeout = session->keep_alive_timeout();
	switch (timeout)
	{
	default:
		mpoller_set_timeout(entry->sockfd, timeout, this->mpoller);
		pthread_mutex_lock(&service->mutex);
		if (service->listen_fd >= 0)
		{
			entry->state = CONN_STATE_KEEPALIVE;
			list_add_tail(&entry->list, &service->alive_list);
			entry = NULL;
		}

		pthread_mutex_unlock(&service->mutex);
		if (entry)
		{
	case 0:
			mpoller_del(entry->sockfd, this->mpoller);
			entry->state = CONN_STATE_CLOSING;
		}
	}
}
```

- 第三部分

没有service的时候

```cpp
else   // if(!service)
{
	if (entry->state == CONN_STATE_IDLE)
	{
		timeout = session->first_timeout();
		if (timeout == 0)
			timeout = Communicator::first_timeout_recv(session);
		else
		{
			session->timeout = -1;
			session->begin_time.tv_nsec = -1;
		}

		mpoller_set_timeout(entry->sockfd, timeout, this->mpoller);
	}

	entry->state = CONN_STATE_RECEIVING;
}

```

## 无可复用的连接

```cpp
entry = this->launch_conn(session, target);

session->conn = entry->conn;
session->seq = entry->seq++;
data.operation = PD_OP_CONNECT;
data.fd = entry->sockfd;
data.ssl = NULL;
data.context = entry;
timeout = session->connect_timeout();
mpoller_add(&data, timeout, this->mpoller);
...
```

如果没有可以复用的连接，我们先去建立连接，然后把connect操作挂到epoll上面监听(异步connect)

```cpp
struct CommConnEntry *Communicator::launch_conn(CommSession *session,
												CommTarget *target)
{
	//1. connect 建立连接
	sockfd = this->nonblock_connect(target);

	entry = (struct CommConnEntry *)malloc(sizeof (struct CommConnEntry));

	pthread_mutex_init(&entry->mutex, NULL);

	//2. 创建新的CommConnection
	// 然后初始化entry
	entry->conn = target->new_connection(sockfd);
	entry->seq = 0;
	entry->mpoller = this->mpoller;
	entry->service = NULL;
	entry->target = target;
	entry->session = session;
	entry->ssl = NULL;
	entry->sockfd = sockfd;
	entry->state = CONN_STATE_CONNECTING;
	entry->ref = 1;
}
```

### connect建立连接

```cpp

int Communicator::nonblock_connect(CommTarget *target)
{
	// 创建cfd
	int sockfd = target->create_connect_fd();
	...
	// 设置非阻塞
	__set_fd_nonblock(sockfd)
	...
	// 然后调用connec连接
	if (connect(sockfd, target->addr, target->addrlen) >= 0 ||
		errno == EINPROGRESS)
	{
		return sockfd;
	}
	...
}
```

```cpp
virtual int create_connect_fd()
{
	return socket(this->addr->sa_family, SOCK_STREAM, 0);
}
```

### 创建新的CommConnection

```cpp
virtual CommConnection *new_connection(int connect_fd)
{
	return new CommConnection;
}
```

### 异步connect

然后我们poller检测出这个事件后

```cpp
// __poller_thread_routines 中调用 __poller_handle_connect(node, poller);

static void __poller_handle_connect(struct __poller_node *node,
									poller_t *poller)
{
	socklen_t len = sizeof (int);
	int error;

	if (getsockopt(node->data.fd, SOL_SOCKET, SO_ERROR, &error, &len) < 0)
		error = errno;

	if (__poller_remove_node(node, poller))
		return;
	...

	poller->cb((struct poller_result *)node, poller->ctx);
}
```

### poller cb

这个在`poller_create` 的是便设置好 `poller->cb = params->callback;`

```cpp
struct poller_params
{
	...
	void (*callback)(struct poller_result *, void *);  
	void *context;
};
```

poller一切结果都通过callback返回

callback的第二个参数void *是poller_params里的context。

poller_start时，poller内部会创建一个线程进行epoll操作。callback是在这个线程里调起的。

因此，callback是在同一个线程里串行执行的。

因为一个读操作会得到很多条SUCCESS消息，保序的callback是必须的。

传给callback的poller_result *，没有const修饰，这个result指针是需要用户free的。

这么做的目的是减少malloc和free次数，以及减少不必要的内存拷贝。

此外，callback里的poller_result结尾处，有6个指针的空间可以利用，相当于你其实得到这个的一个结构：

```cpp
struct poller_result_EXT
{
	int state;
	int error;
	struct poller_data data;
	char avail[6 * sizeof (void *)];  
};
```

一切都是为了减少内存分配释放。总之最重要的是记得要自行free。

而这个callback在此处传入

```cpp
int Communicator::create_poller(size_t poller_threads)
{
	struct poller_params params = {
		.max_open_files		=	65536,
		.create_message		=	Communicator::create_message,
		.partial_written	=	Communicator::partial_written,
		.callback			=	Communicator::callback,
		.context			=	this
	};
	...
	this->mpoller = mpoller_create(&params, poller_threads);
	...
}
```

注意此处的`context = Communicator`

```cpp
void Communicator::callback(struct poller_result *res, void *context)
{
	Communicator *comm = (Communicator *)context;
	msgqueue_put(res, comm->queue);
}
```

所以我们回过头看

```cpp
poller->cb((struct poller_result *)node, poller->ctx);
```

这个就是把node(res) 放入这个msg queue里

这里就是到了`handler_threads`中的生产者(put)-消费者(get)模式了

所以我们把这个结果加到msg queue里，等待消费(get)

### Communicator::handler_thread_routine

```cpp

void Communicator::handler_thread_routine(void *context)
{
	Communicator *comm = (Communicator *)context;
	struct poller_result *res;

	while ((res = (struct poller_result *)msgqueue_get(comm->queue)) != NULL)
	{
		switch (res->data.operation)
		{
		...
		case PD_OP_CONNECT:
		case PD_OP_SSL_CONNECT:
			comm->handle_connect_result(res);
			break;
		...
	}
}
```

### Communicator::handle_connect_result

于是来处理connect，发送entry，并把read放到epoll上间监听

```cpp
void Communicator::handle_connect_result(struct poller_result *res)
{
	struct CommConnEntry *entry = (struct CommConnEntry *)res->data.context;
	CommSession *session = entry->session;
	CommTarget *target = entry->target;

	session->out = session->message_out();

	ret = this->send_message(entry);

	res->data.operation = PD_OP_READ;
	res->data.message = NULL;
	timeout = session->first_timeout();
	if (timeout == 0)
		timeout = Communicator::first_timeout_recv(session);
	else
	{
		session->timeout = -1;
		session->begin_time.tv_nsec = -1;
	}
	...
	mpoller_add(&res->data, timeout, this->mpoller);
	...
}
```